{
  "workingPapers": [],
  "publications": {
    "2024": [
      {
        "id": 3,
        "title": "Democratizing Reward Design for Personal and Representative Value-Alignment",
        "authors": ["Carter Blair", "Kate Larson", "Edith Law"],
        "venues": [],
        "tldr": "A conversational system that learns personalized value definitions from users through interactive dialogue and constructs language-based reward models, enabling AI alignment that respects individual perspectives rather than blindly aggregating crowd preferences.",
        "abstract": "Aligning AI agents with human values is challenging due to diverse and subjective notions of values. Standard alignment methods often aggregate crowd feedback, which can result in the suppression of unique or minority preferences. We introduce Interactive-Reflective Dialogue Alignment, a method that iteratively engages users in reflecting on and specifying their subjective value definitions. This system learns individual value definitions through language-model-based preference elicitation and constructs personalized reward models that can be used to align AI behaviour. We evaluated our system through two studies with 30 participants, one focusing on ``respect'' and the other on ethical decision-making in autonomous vehicles. Our findings demonstrate diverse definitions of value-aligned behaviour and show that our system can accurately capture each person's unique understanding. This approach enables personalized alignment and can inform more representative and interpretable collective alignment strategies.",
        "paperLink": "./papers/democratizing_reward_design.pdf",
        "arxivLink": "https://arxiv.org/abs/2410.22203"
      },
      {
        "id": 4,
        "title": "Altared Environments: The Role of Normative Infrastructure in AI Alignment",
        "authors": ["Rakshit Trivedi", "Nikhil Chandak", "Carter Blair", "Atrisha Sarkar", "Tehilla Weltman", "Dylan Hadfield-Menell", "Gillian K Hadfield"],
        "venues": ["Agentic Markets Workshop at ICML 2024"],
        "tldr": "Introduces 'altars' - environmental features that help AI agents learn cooperation by encoding which actions are socially sanctionable, mimicking how humans use normative infrastructure.",
        "abstract": "Cooperation is central to human life, distinguishing humans as ultra-cooperative among mammals. We form stable groups that enhance welfare through mutual protection, knowledge sharing, and economic exchanges. As artificial intelligence gains autonomy in shared environments, ensuring AI agents can engage in cooperative behaviors is crucial. Research in AI views this as an alignment challenge and frames it in terms of embedding norms and values in AI systems. Such an approach, while promising, neglects how humans achieve stable cooperation through normative infrastructure. This infrastructure establishes shared norms enforced by agents who recognize and sanction norm violations. Using multi-agent reinforcement learning (MARL), we investigate the impact of normative infrastructure on agents' learning dynamics and their cooperative abilities in mixed-motive games. We introduce the concept of an altar, an environmental feature that encodes actions deemed sanctionable by a group of agents. Comparing the performance of simple, independent learning agents in environments with and without the altar, we assess the potential of normative infrastructure in facilitating AI agent alignment to foster stable cooperation.",
        "paperLink": "./papers/Altared_Environments.pdf"
      },
      {
        "id": 5,
        "title": "Normative Modules: A Generative Agent Architecture for Learning Norms that Supports Multi-Agent Cooperation",
        "authors": ["Atrisha Sarkar", "Andrei Ioan Muresanu", "Carter Blair", "Aaryam Sharma", "Rakshit S Trivedi", "Gillian K Hadfield"],
        "venues": ["Foundation Models and Game Theory Workshop at Economics and Computation 2024"],
        "tldr": "An architecture that enables LLM-based agents to learn which social institutions are authoritative in their environment, allowing them to coordinate behavior and achieve better cooperation.",
        "abstract": "Generative agents, which implement behaviors using a large language model (LLM) to interpret and evaluate an environment, has demonstrated the capacity to solve complex tasks across many social and technological domains. However, when these agents interact with other agents and humans in presence of social structures such as existing norms, fostering cooperation between them is a fundamental challenge. In this paper, we develop the framework of a 'Normative Module': an architecture designed to enhance cooperation by enabling agents to recognize and adapt to the normative infrastructure of a given environment. We focus on the equilibrium selection aspect of the cooperation problem and inform our agent design based on the existence of classification institutions that implement correlated equilibrium to provide effective resolution of the equilibrium selection problem. Specifically, the normative module enables agents to learn through peer interactions which of multiple candidate institutions in the environment, does a group treat as authoritative. By enabling normative competence in this sense, agents gain ability to coordinate their sanctioning behaviour; coordinated sanctioning behaviour in turn shapes primary behaviour within a social environment, leading to higher average welfare. We design a new environment that supports institutions and evaluate the proposed framework based on two",
        "paperLink": "./papers/normative_module.pdf",
        "arxivLink": "https://arxiv.org/abs/2405.19328"
      },
      {
        "id": 1,
        "title": "Liquid Ensemble Selection For Continual Learning",
        "authors": ["Carter Blair", "Ben Armstrong", "Kate Larson"],
        "venues": ["AAMAS SCaLA Workshop 2024", "Canadian AI 2024"],
        "tldr": "Uses delegative voting to dynamically select which models in an ensemble should learn vs. predict, enabling better continual learning with less catastrophic forgetting.",
        "abstract": "Continual learning aims to enable machine learning models to continually learn from a shifting data distribution without forgetting what has already been learned. Such shifting distributions can be broken into disjoint subsets of related examples; by training each member of an ensemble on a different subset it is possible for the ensemble as a whole to achieve much higher accuracy with less forgetting than a naive model. We address the problem of selecting which models within an ensemble should learn on any given data, and which should predict. By drawing on work from delegative voting we develop an algorithm for using delegation to dynamically select which models in an ensemble are active. We explore a variety of delegation methods and performance metrics, ultimately finding that delegation is able to provide a significant performance boost over naive learning in the face of distribution shifts.",
        "paperLink": "./papers/SCaLA_AAMAS_Liquid_Ensembles.pdf",
        "arxivLink": "https://arxiv.org/abs/2405.07327",
        "presentationLink": "./slides/LiquidEnsembleSelection_CarterBlair.pdf"
      },
      {
        "id": 2,
        "title": "Quantifying Emotional Responses to Immutable Data Characteristics and Designer Choices in Data Visualizations",
        "authors": ["Carter Blair", "Xiyao Wang", "Charles Perin"],
        "venues": ["IEEE VIS 2024"],
        "tldr": "Shows that data characteristics like trend and density trigger emotional responses even in meaningless data, and provides guidelines for designers to control these emotional effects.",
        "abstract": "Emotion is an important factor to consider when designing visualizations as it can impact the amount of trust viewers place in a visualization, how well they can retrieve information and understand the underlying data, and how much they engage with or connect to a visualization. We conducted five crowdsourced experiments to quantify the effects of color, chart type, data trend, data variability and data density on emotion (measured through self-reported arousal and valence). Results from our experiments show that there are multiple design elements which influence the emotion induced by a visualization and, more surprisingly, that certain data characteristics influence the emotion of viewers even when the data has no meaning. In light of these findings, we offer guidelines on how to use color, scale, and chart type to counterbalance and emphasize the emotional impact of immutable data characteristics.",
        "paperLink": "./papers/Quantifying_Emotional_Responses_to_Immutable_Data_Characteristics_and_Designer_Choices_in_Data_Visualizations.pdf",
        "arxivLink": "https://arxiv.org/abs/2407.18427",
        "presentationLink": "https://www.youtube.com/watch?v=gaAm2v-ENKA&t=1535s&ab_channel=IEEEVisualizationConference"
      }
    ]
  }
}